{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pythainlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jodxqQFX5Yr",
        "outputId": "0a8a8c73-0552-4a62-dac9-eaf963cdc15d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pythainlp\n",
            "  Downloading pythainlp-4.0.2-py3-none-any.whl (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from pythainlp) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.4)\n",
            "Installing collected packages: pythainlp\n",
            "Successfully installed pythainlp-4.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NOxSD0FWQiM",
        "outputId": "a83ddf23-3b1b-469f-d5a8-171355f9d86d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-26 08:40:51--  https://docs.google.com/uc?export=download&id=1K0VgdwYagNidu5k_y5pnSrGrGIOBPsyS\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.193.138, 172.217.193.113, 172.217.193.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.193.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-10-90-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/sdum0fiu92o4e79bp2v752db5g359mon/1690360800000/07034668329300256193/*/1K0VgdwYagNidu5k_y5pnSrGrGIOBPsyS?e=download&uuid=e6eeab66-ecaf-432e-a9e6-ae3cd317c66f [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-07-26 08:40:52--  https://doc-10-90-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/sdum0fiu92o4e79bp2v752db5g359mon/1690360800000/07034668329300256193/*/1K0VgdwYagNidu5k_y5pnSrGrGIOBPsyS?e=download&uuid=e6eeab66-ecaf-432e-a9e6-ae3cd317c66f\n",
            "Resolving doc-10-90-docs.googleusercontent.com (doc-10-90-docs.googleusercontent.com)... 142.251.162.132, 2607:f8b0:400c:c38::84\n",
            "Connecting to doc-10-90-docs.googleusercontent.com (doc-10-90-docs.googleusercontent.com)|142.251.162.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 348315 (340K) [text/csv]\n",
            "Saving to: ‘Fastfood-Opinion.csv’\n",
            "\n",
            "Fastfood-Opinion.cs 100%[===================>] 340.15K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2023-07-26 08:40:52 (92.3 MB/s) - ‘Fastfood-Opinion.csv’ saved [348315/348315]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1K0VgdwYagNidu5k_y5pnSrGrGIOBPsyS' -O Fastfood-Opinion.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic"
      ],
      "metadata": {
        "id": "05FRcvWbXBzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Step 1: Load and preprocess the data\n",
        "data = pd.read_csv('Fastfood-Opinion.csv')\n",
        "X = data['message'].astype(str)\n",
        "y = data['class']\n",
        "\n",
        "# Tokenize Thai text\n",
        "X_tokenized = X.apply(word_tokenize, keep_whitespace=False)\n",
        "\n",
        "vocab = np.array(vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "Eoo7jkavXE9C"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CountVectorizer to transform the text data into bag-of-word features (bow).\n",
        "# Step 2: Create bag-of-words representation\n",
        "\n",
        "#bag\n",
        "vectorizer = CountVectorizer(analyzer=lambda x: x)  # Use the list of tokens as the analyzer\n",
        "X_bow = vectorizer.fit_transform(X_tokenized)\n",
        "# // (documents, vocab) EX. (0, 1069) : 2 // row:0, vocab:1069, has 2 duplicated\n",
        "\n",
        "#big\n",
        "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
        "X_big = bigram_vectorizer.fit_transform(X)\n",
        "\n",
        "#bin\n",
        "binary_vectorizer = CountVectorizer(analyzer=lambda x: x, binary=True)\n",
        "X_bin = binary_vectorizer.fit_transform(X_tokenized)\n"
      ],
      "metadata": {
        "id": "Zw4Eu7RCzHSC"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "representations = {\n",
        "    'bag of word': [X_bow, vectorizer],\n",
        "    'bigram': [X_big, bigram_vectorizer],\n",
        "    'binary': [X_bin, binary_vectorizer]\n",
        "}\n",
        "\n",
        "for label, rep in representations.items():\n",
        "\n",
        "  # Step 3: Split the dataset\n",
        "  X_train, X_test, y_train, y_test = train_test_split(rep[0], y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Step 4: Train the binomial logistic regression\n",
        "  logreg_classifier = LogisticRegression()\n",
        "  logreg_classifier.fit(X_train, y_train)\n",
        "  y_pred = logreg_classifier.predict(X_test)\n",
        "\n",
        "  # Step 5: Evaluate the model\n",
        "  # Print classification report (precision, recall, F1-score, support)\n",
        "  print(f\"\\nClassification Report    ({label}):\")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  # Predict New Data\n",
        "  new_text = pd.Series(\"รสชาติแย่มากครับรู้ตัวมั้ย ไหนจะเรื่องพนักงานบริห่วยแตกแถมยัง service charge อย่างแพงอีก รู้อย่างนี้ไม่น่ามาเลยเสียเที่ยวจริงๆ\")\n",
        "\n",
        "  if label != 'bigram' :\n",
        "    new_text = new_text.apply(word_tokenize, keep_whitespace=False)\n",
        "  new_text_bow = rep[1].transform(new_text)\n",
        "\n",
        "  predicted_class = logreg_classifier.predict(new_text_bow)\n",
        "  print(\"Predicted Class:\", predicted_class[0]) # 0:bad response, 1:good response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8tJYw2kzaPo",
        "outputId": "2c1bedf6-dc35-473c-a536-4c5524d68658"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report    (bag of word):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.86      0.91        74\n",
            "           1       0.84      0.95      0.89        55\n",
            "\n",
            "    accuracy                           0.90       129\n",
            "   macro avg       0.90      0.91      0.90       129\n",
            "weighted avg       0.91      0.90      0.90       129\n",
            "\n",
            "Predicted Class: 0\n",
            "\n",
            "Classification Report    (bigram):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.53      0.68        74\n",
            "           1       0.61      0.98      0.75        55\n",
            "\n",
            "    accuracy                           0.72       129\n",
            "   macro avg       0.79      0.75      0.72       129\n",
            "weighted avg       0.82      0.72      0.71       129\n",
            "\n",
            "Predicted Class: 1\n",
            "\n",
            "Classification Report    (binary):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.89      0.92        74\n",
            "           1       0.87      0.95      0.90        55\n",
            "\n",
            "    accuracy                           0.91       129\n",
            "   macro avg       0.91      0.92      0.91       129\n",
            "weighted avg       0.92      0.91      0.92       129\n",
            "\n",
            "Predicted Class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive"
      ],
      "metadata": {
        "id": "VTas_hQBpbTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "data = pd.read_csv('Fastfood-Opinion.csv')\n",
        "X = data['message'].astype(str)\n",
        "y = data['class']\n",
        "\n",
        "X_tokenized = X.apply(word_tokenize, keep_whitespace=False)\n",
        "\n",
        "#bag\n",
        "vectorizer2 = CountVectorizer(analyzer=lambda x: x)\n",
        "X_bow = vectorizer2.fit_transform(X_tokenized)\n",
        "\n",
        "#big\n",
        "bigram_vectorizer2 = CountVectorizer(ngram_range=(2, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
        "X_big = bigram_vectorizer2.fit_transform(X)\n",
        "\n",
        "#bin\n",
        "binary_vectorizer2 = CountVectorizer(analyzer=lambda x: x, binary=True)\n",
        "X_bin = binary_vectorizer2.fit_transform(X_tokenized)\n",
        "\n",
        "representations = {\n",
        "    'bag of word': [X_bow, vectorizer2],\n",
        "    'bigram': [X_big, bigram_vectorizer2],\n",
        "    'binary': [X_bin, binary_vectorizer2]\n",
        "}\n",
        "\n",
        "for label, rep in representations.items():\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(rep[0], y, test_size=0.2, random_state=42)\n",
        "\n",
        "  nb_classifier = MultinomialNB()\n",
        "  nb_classifier.fit(X_train, y_train)\n",
        "  y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "  print(f\"\\nClassification Report    ({label}):\")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  new_text = pd.Series(\"รสชาติแย่มากครับรู้ตัวมั้ย ไหนจะเรื่องพนักงานบริห่วยแตกแถมยัง service charge อย่างแพงอีก รู้อย่างนี้ไม่น่ามาเลยเสียเที่ยวจริงๆ\")\n",
        "\n",
        "  if label != 'bigram' :\n",
        "    new_text = new_text.apply(word_tokenize, keep_whitespace=False)\n",
        "  new_text_bow = rep[1].transform(new_text)\n",
        "\n",
        "  predicted_class = nb_classifier.predict(new_text_bow)\n",
        "  print(\"Predicted Class:\", predicted_class[0])\n",
        "  print(\"-------------------------------------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9B0EH0wpeJo",
        "outputId": "65131659-c5a5-4bec-fb07-32438a9397c7"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report    (bag of word):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.96      0.91        74\n",
            "           1       0.94      0.80      0.86        55\n",
            "\n",
            "    accuracy                           0.89       129\n",
            "   macro avg       0.90      0.88      0.89       129\n",
            "weighted avg       0.90      0.89      0.89       129\n",
            "\n",
            "Predicted Class: 0\n",
            "-------------------------------------------------------------\n",
            "\n",
            "Classification Report    (bigram):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.11      0.19        74\n",
            "           1       0.45      0.98      0.62        55\n",
            "\n",
            "    accuracy                           0.48       129\n",
            "   macro avg       0.67      0.54      0.40       129\n",
            "weighted avg       0.70      0.48      0.37       129\n",
            "\n",
            "Predicted Class: 0\n",
            "-------------------------------------------------------------\n",
            "\n",
            "Classification Report    (binary):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.95      0.90        74\n",
            "           1       0.91      0.78      0.84        55\n",
            "\n",
            "    accuracy                           0.88       129\n",
            "   macro avg       0.88      0.86      0.87       129\n",
            "weighted avg       0.88      0.88      0.87       129\n",
            "\n",
            "Predicted Class: 0\n",
            "-------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze and Conclude section"
      ],
      "metadata": {
        "id": "5GXuYRP3VBvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. วิเคราะห์ผลลัพธ์ที่ได้ bag, bin, big\n",
        " -  bin มีความเป็นไปได้ว่าจะสามารถจัดกลุ่ม vocab ได้มีประสิทธิภาพมากที่สุด เมื่อสังเกตุจาก accuraccy และรองลงมาเป็น bag, big\n",
        "2. ทดลองใส่ประโยคใหม่เพื่อทดสอบการทำงานของโมเดล\n",
        "และวิเคราะห์ผลลัพธ์ที่ได้\n",
        " - จากที่ได้เห็นดูเหมือนว่า bin, bag จะสามารถตึความและ predict class ออกมาได้ตรง แต่กลับกัน big วิเคราะห์ intension ผิดแปลกไปซึ่งเป็นไปตามค่า accuracy ที่ถดถอยลงตามลำดับ"
      ],
      "metadata": {
        "id": "hQS5OV-rVQxA"
      }
    }
  ]
}